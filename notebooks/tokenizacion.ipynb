{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316c44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b692cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import cargar_corpus, tokenizar_por_espacios, tokenizar_con_bpe, construir_vocabularios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9527e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola, ¿cómo estás?', 'El clima está muy agradable hoy.', 'Mi nombre es Gabriela.', 'Estoy estudiando procesamiento de lenguaje natural.', 'Los modelos de lenguaje son muy interesantes.', '¿Qué planes tienes para el fin de semana?', 'La inteligencia artificial está avanzando rápidamente.', 'Estoy aprendiendo a programar en Python.', 'Me gusta leer libros de ciencia ficción.', '¿Quieres ir al cine esta noche?', 'Estoy escribiendo mi tesis universitaria.', 'Los transformers cambiaron el campo del NLP.', 'Los embeddings permiten representar el significado.', 'Byte-Pair Encoding es una técnica de compresión.', 'Estoy entrenando un modelo con Hugging Face.', 'La tokenización afecta el resultado del modelo.', 'La matriz de similitud mide la cercanía semántica.', 'PCA ayuda a visualizar datos en 2D.', 'La distancia coseno mide la similitud angular.', 'Este proyecto fue desarrollado por Gabriela Colque.']\n"
     ]
    }
   ],
   "source": [
    "# Cargar el corpus\n",
    "corpus = cargar_corpus(\"../data/corpus.txt\")\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803a332",
   "metadata": {},
   "source": [
    "Tokenizarlo por espacios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "575eafff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hola', 'como', 'estas'], ['el', 'clima', 'esta', 'muy', 'agradable', 'hoy'], ['mi', 'nombre', 'es', 'gabriela'], ['estoy', 'estudiando', 'procesamiento', 'de', 'lenguaje', 'natural'], ['los', 'modelos', 'de', 'lenguaje', 'son', 'muy', 'interesantes'], ['que', 'planes', 'tienes', 'para', 'el', 'fin', 'de', 'semana'], ['la', 'inteligencia', 'artificial', 'esta', 'avanzando', 'rapidamente'], ['estoy', 'aprendiendo', 'a', 'programar', 'en', 'python'], ['me', 'gusta', 'leer', 'libros', 'de', 'ciencia', 'ficcion'], ['quieres', 'ir', 'al', 'cine', 'esta', 'noche'], ['estoy', 'escribiendo', 'mi', 'tesis', 'universitaria'], ['los', 'transformers', 'cambiaron', 'el', 'campo', 'del', 'nlp'], ['los', 'embeddings', 'permiten', 'representar', 'el', 'significado'], ['bytepair', 'encoding', 'es', 'una', 'tecnica', 'de', 'compresion'], ['estoy', 'entrenando', 'un', 'modelo', 'con', 'hugging', 'face'], ['la', 'tokenizacion', 'afecta', 'el', 'resultado', 'del', 'modelo'], ['la', 'matriz', 'de', 'similitud', 'mide', 'la', 'cercania', 'semantica'], ['pca', 'ayuda', 'a', 'visualizar', 'datos', 'en', '2d'], ['la', 'distancia', 'coseno', 'mide', 'la', 'similitud', 'angular'], ['este', 'proyecto', 'fue', 'desarrollado', 'por', 'gabriela', 'colque']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar el corpus por espacios\n",
    "corpus_tokenizado = tokenizar_por_espacios(corpus)\n",
    "print(corpus_tokenizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "950d0156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hola': 0, 'como': 1, 'estas': 2, 'el': 3, 'clima': 4, 'esta': 5, 'muy': 6, 'agradable': 7, 'hoy': 8, 'mi': 9, 'nombre': 10, 'es': 11, 'gabriela': 12, 'estoy': 13, 'estudiando': 14, 'procesamiento': 15, 'de': 16, 'lenguaje': 17, 'natural': 18, 'los': 19, 'modelos': 20, 'son': 21, 'interesantes': 22, 'que': 23, 'planes': 24, 'tienes': 25, 'para': 26, 'fin': 27, 'semana': 28, 'la': 29, 'inteligencia': 30, 'artificial': 31, 'avanzando': 32, 'rapidamente': 33, 'aprendiendo': 34, 'a': 35, 'programar': 36, 'en': 37, 'python': 38, 'me': 39, 'gusta': 40, 'leer': 41, 'libros': 42, 'ciencia': 43, 'ficcion': 44, 'quieres': 45, 'ir': 46, 'al': 47, 'cine': 48, 'noche': 49, 'escribiendo': 50, 'tesis': 51, 'universitaria': 52, 'transformers': 53, 'cambiaron': 54, 'campo': 55, 'del': 56, 'nlp': 57, 'embeddings': 58, 'permiten': 59, 'representar': 60, 'significado': 61, 'bytepair': 62, 'encoding': 63, 'una': 64, 'tecnica': 65, 'compresion': 66, 'entrenando': 67, 'un': 68, 'modelo': 69, 'con': 70, 'hugging': 71, 'face': 72, 'tokenizacion': 73, 'afecta': 74, 'resultado': 75, 'matriz': 76, 'similitud': 77, 'mide': 78, 'cercania': 79, 'semantica': 80, 'pca': 81, 'ayuda': 82, 'visualizar': 83, 'datos': 84, '2d': 85, 'distancia': 86, 'coseno': 87, 'angular': 88, 'este': 89, 'proyecto': 90, 'fue': 91, 'desarrollado': 92, 'por': 93, 'colque': 94}\n",
      "{0: 'hola', 1: 'como', 2: 'estas', 3: 'el', 4: 'clima', 5: 'esta', 6: 'muy', 7: 'agradable', 8: 'hoy', 9: 'mi', 10: 'nombre', 11: 'es', 12: 'gabriela', 13: 'estoy', 14: 'estudiando', 15: 'procesamiento', 16: 'de', 17: 'lenguaje', 18: 'natural', 19: 'los', 20: 'modelos', 21: 'son', 22: 'interesantes', 23: 'que', 24: 'planes', 25: 'tienes', 26: 'para', 27: 'fin', 28: 'semana', 29: 'la', 30: 'inteligencia', 31: 'artificial', 32: 'avanzando', 33: 'rapidamente', 34: 'aprendiendo', 35: 'a', 36: 'programar', 37: 'en', 38: 'python', 39: 'me', 40: 'gusta', 41: 'leer', 42: 'libros', 43: 'ciencia', 44: 'ficcion', 45: 'quieres', 46: 'ir', 47: 'al', 48: 'cine', 49: 'noche', 50: 'escribiendo', 51: 'tesis', 52: 'universitaria', 53: 'transformers', 54: 'cambiaron', 55: 'campo', 56: 'del', 57: 'nlp', 58: 'embeddings', 59: 'permiten', 60: 'representar', 61: 'significado', 62: 'bytepair', 63: 'encoding', 64: 'una', 65: 'tecnica', 66: 'compresion', 67: 'entrenando', 68: 'un', 69: 'modelo', 70: 'con', 71: 'hugging', 72: 'face', 73: 'tokenizacion', 74: 'afecta', 75: 'resultado', 76: 'matriz', 77: 'similitud', 78: 'mide', 79: 'cercania', 80: 'semantica', 81: 'pca', 82: 'ayuda', 83: 'visualizar', 84: 'datos', 85: '2d', 86: 'distancia', 87: 'coseno', 88: 'angular', 89: 'este', 90: 'proyecto', 91: 'fue', 92: 'desarrollado', 93: 'por', 94: 'colque'}\n"
     ]
    }
   ],
   "source": [
    "# Construir vocabularios a partir de tokenización por espacios\n",
    "vocab, vocab_inv = construir_vocabularios(corpus_tokenizado)\n",
    "print(vocab)\n",
    "print(vocab_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f549ff",
   "metadata": {},
   "source": [
    "Tokenizarlo con BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b935c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁', 'H', 'o', 'l', 'a', ',', '▁', '¿', 'c', 'ó', 'm', 'o', '▁est', 'á', 's', '?'], ['▁E', 'l', '▁c', 'l', 'im', 'a', '▁est', 'á', '▁m', 'u', 'y', '▁a', 'g', 'r', 'a', 'd', 'a', 'b', 'l', 'e', '▁', 'h', 'o', 'y', '.'], ['▁', 'M', 'i', '▁', 'n', 'o', 'm', 'br', 'e', '▁es', '▁', 'G', 'a', 'br', 'i', 'el', 'a', '.'], ['▁Estoy', '▁est', 'ud', 'i', 'an', 'do', '▁p', 'ro', 'c', 'es', 'am', 'ien', 'to', '▁de', '▁l', 'en', 'gu', 'a', 'j', 'e', '▁', 'n', 'a', 't', 'u', 'r', 'a', 'l', '.'], ['▁L', 'os', '▁m', 'o', 'de', 'l', 'os', '▁de', '▁l', 'en', 'gu', 'a', 'j', 'e', '▁s', 'on', '▁m', 'u', 'y', '▁', 'in', 't', 'er', 'es', 'an', 't', 'es', '.'], ['▁', '¿', 'Q', 'u', 'é', '▁p', 'l', 'an', 'es', '▁t', 'ien', 'es', '▁p', 'ar', 'a', '▁el', '▁f', 'in', '▁de', '▁s', 'em', 'an', 'a', '?'], ['▁La', '▁', 'in', 't', 'el', 'i', 'g', 'en', 'cia', '▁', 'ar', 't', 'i', 'f', 'i', 'cia', 'l', '▁est', 'á', '▁a', 'v', 'an', 'z', 'an', 'do', '▁', 'r', 'á', 'p', 'i', 'd', 'am', 'en', 'te', '.'], ['▁Estoy', '▁a', 'pr', 'en', 'd', 'ien', 'do', '▁a', '▁p', 'ro', 'g', 'r', 'am', 'ar', '▁', 'en', '▁', 'P', 'y', 't', 'h', 'on', '.'], ['▁', 'M', 'e', '▁', 'gu', 's', 'ta', '▁l', 'e', 'er', '▁l', 'i', 'br', 'os', '▁de', '▁c', 'ien', 'cia', '▁f', 'ic', 'ci', 'ón', '.'], ['▁', '¿', 'Q', 'u', 'i', 'er', 'es', '▁', 'i', 'r', '▁a', 'l', '▁c', 'in', 'e', '▁est', 'a', '▁', 'n', 'o', 'c', 'h', 'e', '?'], ['▁Estoy', '▁es', 'c', 'r', 'i', 'b', 'ien', 'do', '▁m', 'i', '▁t', 'es', 'is', '▁', 'un', 'i', 'v', 'er', 's', 'it', 'ar', 'i', 'a', '.'], ['▁L', 'os', '▁t', 'r', 'an', 's', 'f', 'o', 'r', 'm', 'er', 's', '▁c', 'am', 'b', 'i', 'ar', 'on', '▁el', '▁c', 'am', 'p', 'o', '▁de', 'l', '▁', 'N', 'L', 'P', '.'], ['▁L', 'os', '▁', 'em', 'b', 'e', 'd', 'd', 'in', 'g', 's', '▁p', 'er', 'm', 'it', 'en', '▁', 'r', 'e', 'pr', 'es', 'en', 't', 'ar', '▁el', '▁s', 'i', 'g', 'n', 'i', 'f', 'ic', 'a', 'do', '.'], ['▁', 'B', 'y', 'te', '-', 'P', 'a', 'i', 'r', '▁E', 'n', 'c', 'o', 'd', 'in', 'g', '▁es', '▁', 'un', 'a', '▁t', 'é', 'c', 'n', 'ic', 'a', '▁de', '▁c', 'o', 'm', 'pr', 'es', 'i', 'ón', '.'], ['▁Estoy', '▁', 'en', 't', 'r', 'en', 'an', 'do', '▁', 'un', '▁m', 'o', 'de', 'l', 'o', '▁c', 'on', '▁', 'H', 'u', 'g', 'g', 'in', 'g', '▁', 'F', 'a', 'c', 'e', '.'], ['▁La', '▁', 'to', 'k', 'en', 'iz', 'a', 'ci', 'ón', '▁a', 'f', 'e', 'c', 'ta', '▁el', '▁', 'r', 'es', 'u', 'l', 'ta', 'do', '▁de', 'l', '▁m', 'o', 'de', 'l', 'o', '.'], ['▁La', '▁m', 'a', 't', 'r', 'iz', '▁de', '▁s', 'im', 'i', 'l', 'it', 'ud', '▁m', 'i', 'de', '▁l', 'a', '▁c', 'er', 'c', 'an', 'í', 'a', '▁s', 'em', 'á', 'n', 't', 'ic', 'a', '.'], ['▁', 'P', 'C', 'A', '▁a', 'y', 'ud', 'a', '▁a', '▁', 'v', 'is', 'u', 'a', 'l', 'iz', 'ar', '▁d', 'a', 'to', 's', '▁', 'en', '▁', '2', 'D', '.'], ['▁La', '▁d', 'is', 't', 'an', 'cia', '▁c', 'os', 'en', 'o', '▁m', 'i', 'de', '▁l', 'a', '▁s', 'im', 'i', 'l', 'it', 'ud', '▁', 'an', 'gu', 'l', 'ar', '.'], ['▁Es', 'te', '▁p', 'ro', 'y', 'e', 'c', 'to', '▁f', 'u', 'e', '▁d', 'es', 'ar', 'ro', 'l', 'l', 'a', 'do', '▁p', 'o', 'r', '▁', 'G', 'a', 'br', 'i', 'el', 'a', '▁', 'C', 'o', 'l', 'q', 'u', 'e', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar el corpus usando BPE\n",
    "corpus_tokenizado_bpe = tokenizar_con_bpe(corpus, \"../data/bpe_model.model\")\n",
    "print(corpus_tokenizado_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68d7f51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'▁': 0, 'H': 1, 'o': 2, 'l': 3, 'a': 4, ',': 5, '¿': 6, 'c': 7, 'ó': 8, 'm': 9, '▁est': 10, 'á': 11, 's': 12, '?': 13, '▁E': 14, '▁c': 15, 'im': 16, '▁m': 17, 'u': 18, 'y': 19, '▁a': 20, 'g': 21, 'r': 22, 'd': 23, 'b': 24, 'e': 25, 'h': 26, '.': 27, 'M': 28, 'i': 29, 'n': 30, 'br': 31, '▁es': 32, 'G': 33, 'el': 34, '▁Estoy': 35, 'ud': 36, 'an': 37, 'do': 38, '▁p': 39, 'ro': 40, 'es': 41, 'am': 42, 'ien': 43, 'to': 44, '▁de': 45, '▁l': 46, 'en': 47, 'gu': 48, 'j': 49, 't': 50, '▁L': 51, 'os': 52, 'de': 53, '▁s': 54, 'on': 55, 'in': 56, 'er': 57, 'Q': 58, 'é': 59, '▁t': 60, 'ar': 61, '▁el': 62, '▁f': 63, 'em': 64, '▁La': 65, 'cia': 66, 'f': 67, 'v': 68, 'z': 69, 'p': 70, 'te': 71, 'pr': 72, 'P': 73, 'ta': 74, 'ic': 75, 'ci': 76, 'ón': 77, 'is': 78, 'un': 79, 'it': 80, 'N': 81, 'L': 82, 'B': 83, '-': 84, 'F': 85, 'k': 86, 'iz': 87, 'í': 88, 'C': 89, 'A': 90, '▁d': 91, '2': 92, 'D': 93, '▁Es': 94, 'q': 95}\n",
      "{0: '▁', 1: 'H', 2: 'o', 3: 'l', 4: 'a', 5: ',', 6: '¿', 7: 'c', 8: 'ó', 9: 'm', 10: '▁est', 11: 'á', 12: 's', 13: '?', 14: '▁E', 15: '▁c', 16: 'im', 17: '▁m', 18: 'u', 19: 'y', 20: '▁a', 21: 'g', 22: 'r', 23: 'd', 24: 'b', 25: 'e', 26: 'h', 27: '.', 28: 'M', 29: 'i', 30: 'n', 31: 'br', 32: '▁es', 33: 'G', 34: 'el', 35: '▁Estoy', 36: 'ud', 37: 'an', 38: 'do', 39: '▁p', 40: 'ro', 41: 'es', 42: 'am', 43: 'ien', 44: 'to', 45: '▁de', 46: '▁l', 47: 'en', 48: 'gu', 49: 'j', 50: 't', 51: '▁L', 52: 'os', 53: 'de', 54: '▁s', 55: 'on', 56: 'in', 57: 'er', 58: 'Q', 59: 'é', 60: '▁t', 61: 'ar', 62: '▁el', 63: '▁f', 64: 'em', 65: '▁La', 66: 'cia', 67: 'f', 68: 'v', 69: 'z', 70: 'p', 71: 'te', 72: 'pr', 73: 'P', 74: 'ta', 75: 'ic', 76: 'ci', 77: 'ón', 78: 'is', 79: 'un', 80: 'it', 81: 'N', 82: 'L', 83: 'B', 84: '-', 85: 'F', 86: 'k', 87: 'iz', 88: 'í', 89: 'C', 90: 'A', 91: '▁d', 92: '2', 93: 'D', 94: '▁Es', 95: 'q'}\n"
     ]
    }
   ],
   "source": [
    "# Construir vocabularios a partir de BPE\n",
    "vocab_bpe, vocab_bpe_inv = construir_vocabularios(corpus_tokenizado_bpe)\n",
    "print(vocab_bpe)\n",
    "print(vocab_bpe_inv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
