{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75684b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea0d57d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas\n",
    "corpus_path = \"../data/corpus.txt\"\n",
    "model_prefix = \"../data/bpe_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66a06fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: ../data/bpe_model\n",
      "  model_type: BPE\n",
      "  vocab_size: 50\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: -1\n",
      "  eos_id: -1\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ../data/corpus.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 20 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=851\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=48\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 20 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 20\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 101\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: ../data/bpe_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: ../data/bpe_model.vocab\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo BPE\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=corpus_path,\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=50,\n",
    "    model_type=\"bpe\",\n",
    "    character_coverage=1.0,\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=-1,\n",
    "    eos_id=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18927d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frase original: Hola, ¿cómo estás?\n",
      "Subpalabras: ['▁', 'H', 'o', 'l', 'a', ',', '▁', '¿', 'c', 'ó', 'm', 'o', '▁', 'e', 's', 't', 'á', 's', '?']\n",
      "\n",
      "Frase original: El clima está muy agradable hoy.\n",
      "Subpalabras: ['▁', 'E', 'l', '▁', 'c', 'l', 'i', 'm', 'a', '▁', 'e', 's', 't', 'á', '▁', 'm', 'u', 'y', '▁', 'a', 'g', 'r', 'a', 'd', 'a', 'b', 'l', 'e', '▁', 'h', 'o', 'y', '.']\n",
      "\n",
      "Frase original: Mi nombre es Gabriela.\n",
      "Subpalabras: ['▁', 'M', 'i', '▁', 'n', 'o', 'm', 'b', 'r', 'e', '▁', 'e', 's', '▁', 'G', 'a', 'b', 'r', 'i', 'e', 'l', 'a', '.']\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo entrenado\n",
    "sp = spm.SentencePieceProcessor(model_file=f\"{model_prefix}.model\")\n",
    "\n",
    "# Probar tokenización con frases del corpus\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    frases = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "for frase in frases[:3]:\n",
    "    print(f\"\\nFrase original: {frase}\")\n",
    "    print(\"Subpalabras:\", sp.encode(frase, out_type=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71fcf0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frase original: Hola, ¿cómo estás?\n",
      "Subpalabras: ['▁', 'H', 'o', 'l', 'a', ',', '▁', '¿', 'c', 'ó', 'm', 'o', '▁est', 'á', 's', '?']\n",
      "\n",
      "Frase original: El clima está muy agradable hoy.\n",
      "Subpalabras: ['▁E', 'l', '▁c', 'l', 'im', 'a', '▁est', 'á', '▁m', 'u', 'y', '▁a', 'g', 'r', 'a', 'd', 'a', 'b', 'l', 'e', '▁', 'h', 'o', 'y', '.']\n",
      "\n",
      "Frase original: Mi nombre es Gabriela.\n",
      "Subpalabras: ['▁', 'M', 'i', '▁', 'n', 'o', 'm', 'br', 'e', '▁es', '▁', 'G', 'a', 'br', 'i', 'el', 'a', '.']\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo entrenado con 100 vocabulario\n",
    "sp = spm.SentencePieceProcessor(model_file=f\"{model_prefix}.model\")\n",
    "\n",
    "# Probar tokenización con frases del corpus\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    frases = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "for frase in frases[:3]:\n",
    "    print(f\"\\nFrase original: {frase}\")\n",
    "    print(\"Subpalabras:\", sp.encode(frase, out_type=str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
